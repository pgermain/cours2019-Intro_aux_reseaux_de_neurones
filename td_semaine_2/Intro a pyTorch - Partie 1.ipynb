{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction à la librairie PyTorch -- Partie 1\n",
    "Matériel de cours rédigé par Pascal Germain, 2019\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import aidecours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La libraire pyTorch\n",
    "\n",
    "https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tenseur peut contenir un scalaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tenseurs contenant des vecteurs ou des matrices se comportent similairement aux *array numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1,2,3])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([1.,2.,3.])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(u) # Conversion d'un «tensor» pytorch en un «array» numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1.,2.,3.], [4, 5, 6]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * M + 1 # Opérations sur les élements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M @ u # Produit matriciel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((3, 4)) # Matrice de nombres aléatoires tirés uniformément dans l'intervalle [0,1]\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((3, 4)) # Matrice de nombres aléatoires tirés uniformément selon une loi normale N(0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION:** Les *tenseurs pyTorch* sont plus capricieux quant à la gestion des variables de types différents que les *array numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([.3, .6, .9])\n",
    "v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-1, 3, 8])\n",
    "w.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tensor = torch.from_numpy(v)\n",
    "v_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.from_numpy(w)\n",
    "w_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v:', v.dtype)\n",
    "print('w:', w.dtype)\n",
    "\n",
    "result = v @ w\n",
    "print('v @ w:', result.dtype)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', w_tensor.dtype)\n",
    "print('w_tensor:', v_tensor.dtype)\n",
    "result = v_tensor @ w_tensor\n",
    "print('v_tensor @ w_tensor:', result.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.tensor(w, dtype=torch.float64)\n",
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', v_tensor.dtype)\n",
    "print('w_tensor:', w_tensor.dtype)\n",
    "result = v_tensor @ w_tensor\n",
    "print('v_tensor @ x_tensor:', result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dérivation automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'initialisation d'un tenseur, l'argument `requires_grad=True` indique que nous désirons calculer le gradient des variables contenues dans le tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphe de calcul est alors bâti au fur et à mesure que des opérations mathématiques sont appliquées aux tenseurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `F.backward()` parcours le graphe de calcul en sens inverse et calcule le gradient de la fonction $F$ selon chacune des variables du graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir exécuté la fonction `backward()`, l'attribut `grad` des tenseurs impliqués dans le calcul contient la valeur du gradient calculé au point courant. Ici, on aura la valeur :\n",
    "\n",
    "$$\\left[\\frac{\\partial F(x)}{\\partial x}\\right]_{x=3} = \\big[\\,2\\,x\\,\\big]_{x=3} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrons le fonctionnement de la dérivation automatique par quelques autres exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 11, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produit_scalaire = x @ x\n",
    "produit_scalaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produit_scalaire.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = a*b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = 2*a + b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=False)\n",
    "m = a ** b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = torch.tensor(4., requires_grad=True)\n",
    "m1 = (a + b)\n",
    "m2 = m1 * c\n",
    "m2.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)\n",
    "print('c.grad =', c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteur_a = torch.tensor([-1., 2, 3], requires_grad=True)\n",
    "vecteur_b = torch.ones(3, requires_grad=True)\n",
    "produit = vecteur_a @ vecteur_b\n",
    "produit.backward()\n",
    "print('vecteur_a =', vecteur_a, '; vecteur_a.grad =', vecteur_a.grad)\n",
    "print('vecteur_b =', vecteur_b, '; vecteur_b.grad =', vecteur_b.grad)\n",
    "print('produit =', produit.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteur_a = torch.tensor([1., 4, 9], requires_grad=True)\n",
    "result = torch.sum(torch.sqrt(vecteur_a))\n",
    "result.backward()\n",
    "print('vecteur_a =', vecteur_a, '; vecteur_a.grad =', vecteur_a.grad)\n",
    "print('result =', result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencons par un exemple en une dimension.\n",
    "\n",
    "$$f(x) = x^2 - x + 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_maison(x):\n",
    "    return x**2 - x + 2\n",
    "\n",
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, fonction_maison(x) )\n",
    "plt.plot(.5, fonction_maison(.5), 'r*')\n",
    "plt.ylim(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = .4 # Pas de gradient\n",
    "T =  20  # Nombre d'itérations\n",
    "\n",
    "# Initialisation aléatoire \n",
    "x = torch.randn(1, requires_grad=True)\n",
    "\n",
    "for t in range(T):\n",
    " \n",
    "    # Calcul de la fonction objectif\n",
    "    val = fonction_maison(x)\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    val.backward()\n",
    "    \n",
    "    #print('Iteration', t+1, ': x =', x.item(), '; f(x) =', val.item(), '; f\\'(x) =', x.grad.item())\n",
    "    \n",
    "    print(f'Iteration {t+1:02}: x ={x.item(): .5f} ; f(x) ={val.item(): .5f} ; f\\'(x) ={x.grad.item(): .5f}')\n",
    "    \n",
    "    # Mise à jour de la variable x\n",
    "    with torch.no_grad():\n",
    "        x -= eta * x.grad\n",
    "    \n",
    "    # Remise à zéro du gradient\n",
    "    x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons l'exemple des moindres carrés présentés dans les transparents du cours.\n",
    "\n",
    "$$\\min_\\wbf \\left[\\frac1n \\sum_{i=1}^n (\\wbf\\cdot\\xbf_i- y_i)^2\\right].$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moindre_carres_objectif(x, y, w): \n",
    "    return np.mean((x @ w - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(1,1), (0,-1), (2,.5)])\n",
    "y = np.array([-1, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonction_objectif = lambda w: moindre_carres_objectif(x, y, w)\n",
    "aidecours.show_2d_function(fonction_objectif, -5, 5, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "\n",
    "aidecours.show_2d_function(fonction_objectif, -5, 5, .5, optimal=w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons une classe `moindre_carres` avec un fonctionnement semblable aux algorithmes de *scikit-learn* qui résout le problème des moindres carrés par descente de gradient, en utilisant les fonctionnalités de *pyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moindre_carres:\n",
    "    def __init__(self, eta=0.4, nb_iter=50, seed=None):\n",
    "        self.eta = eta\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        \n",
    "        self.w_list = list() # Servira à garder une trace de la descente de gradient\n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            \n",
    "            loss = torch.mean((x @ self.w - y) ** 2)\n",
    "            \n",
    "            self.w_list.append(np.array(self.w.detach()))\n",
    "            self.obj_list.append(loss.item()) \n",
    "            if t == self.nb_iter: break \n",
    "        \n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.w -= self.eta * self.w.grad\n",
    "                \n",
    "            self.w.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = x @ self.w.detach()\n",
    "            \n",
    "        return pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécution de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = moindre_carres(eta, nb_iter)\n",
    "algo.apprentissage(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14.5, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, fonction_objectif, w_opt=w_opt, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1], obj_opt=fonction_objectif(w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Dans cet exercice, nous vous demandons de vous inspirer de la classe `moindre_carrees` ci-haut et de l'adapter au problème de la régression logistique présenté dans les transparents du cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "xx, yy = make_blobs(n_samples=100, centers=2, n_features=2, cluster_std=1, random_state=0)\n",
    "\n",
    "aidecours.show_2d_dataset(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrons la fonction à optimiser (avec $\\lambda=0.01$):\n",
    "    \n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i \\wbf\\cdot\\xbf_i + \\log(1+e^{\\wbf\\cdot\\xbf_i})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def calc_perte_logistique(w, x, y, rho):\n",
    "    pred = sigmoid(x @ w)\n",
    "    pred[y==0] = 1-pred[y==0]\n",
    "    return np.mean(-np.log(pred)) + rho*w @ w/2\n",
    "\n",
    "fct_objectif = lambda w: calc_perte_logistique(w, xx, yy, 0.01)\n",
    "aidecours.show_2d_function(fct_objectif, -4, 4, .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter le code de la classe suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        self.rho = rho\n",
    "        self.eta = eta\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        \n",
    "        self.w_list = list() # Servira à garder une trace de la descente de gradient\n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            pass # Compléter\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = x @ self.w.detach()\n",
    "            \n",
    "        return np.array(pred.numpy() > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter le code suivant pour vérifier le bon fonctionnement de votre algorithme. Essayer ensuite de varier les paramètres `rho`, `eta` et `nb_iter` afin d'évaluer leur impact sur le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.01\n",
    "eta = 0.4     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = regression_logistique(rho, eta, nb_iter)\n",
    "algo.apprentissage(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, fct_objectif, -4, 4, .05, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons l'exercice précédent en ajoutant l'apprentissange d'un *biais* à la régression logistique:\n",
    "\n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i (\\wbf\\cdot\\xbf_i+b) + \\log(1+e^{\\wbf\\cdot\\xbf_i+b})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique_avec_biais:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        self.rho = rho\n",
    "        self.eta = eta\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        \n",
    "        self.w_list = list() # Servira à garder une trace de la descente de gradient\n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            pass # Compléter\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = x @ self.w.detach() + self.b.item()\n",
    "            \n",
    "        return np.array(pred.numpy() > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.01\n",
    "eta = 0.4     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = regression_logistique_avec_biais(rho, eta, nb_iter)\n",
    "algo.apprentissage(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[0])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

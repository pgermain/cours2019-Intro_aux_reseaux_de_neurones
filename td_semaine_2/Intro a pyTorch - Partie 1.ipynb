{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction à la librairie PyTorch -- Partie 1\n",
    "Matériel de cours rédigé par Pascal Germain, 2019\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Référence: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalaires\n",
    "Un tenseur peut contenir un scalaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vecteurs\n",
    "Les tenseurs contenant des vecteurs ou des matrices se comportent similairement aux *array numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1,2,3])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([1.,2.,3.])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1.,2.,3.], [4, 5, 6]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * M + 1 # Opérations sur les élements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M @ u # Produit matriciel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombres aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(5) # Vecteur de nombres aléatoires tirés uniformément dans l'intervalle [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(8) # Vecteur de nombres aléatoires tirés uniformément selon une loi normale N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((3, 4)) # Matrice de nombres aléatoires tirés uniformément dans l'intervalle [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((3, 4)) # Matrice de nombres aléatoires tirés uniformément selon une loi normale N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Initialisation du générateur de nombres aléatoires\n",
    "torch.randn((3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion entre numpy et pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est impossible d'effecture *directement* des opérations arithmétiques entre un «array numpy» et un «tenseur pytorch»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-1, 3, 8])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u = torch.tensor([1,2,3])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w + u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longrightarrow$ pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.from_numpy(w)\n",
    "w_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longleftarrow$ pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_numpy = u.numpy()  # Conversion d'un «tensor» pytorch en un «array» numpy. \n",
    "u_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considération technique 1\n",
    "Dans les deux exemples précédent, les structures de données partagent la même mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w += 1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u *= 2\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longleftarrow$ pytorch ***avec copie mémoire***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_tensor = torch.Tensor(w)\n",
    "ww_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w +=1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longrightarrow$ pytorch ***avec copie mémoire***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_numpy_copie = np.array(u) # Conversion d'un «tensor» pytorch en un «array» numpy. \n",
    "u_numpy_copie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u *= 2\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_numpy_copie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considération technique 2\n",
    "Les *tenseurs pyTorch* sont plus capricieux quant à la gestion des variables de types différents que les *array numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([.3, .6, .9])\n",
    "v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-1, 3, 8])\n",
    "w.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tensor = torch.from_numpy(v)\n",
    "v_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.from_numpy(w)\n",
    "w_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v:', v.dtype)\n",
    "print('w:', w.dtype)\n",
    "\n",
    "result = v @ w\n",
    "print('v @ w:', result.dtype)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', w_tensor.dtype)\n",
    "print('w_tensor:', v_tensor.dtype)\n",
    "result = v_tensor @ w_tensor\n",
    "print('v_tensor @ w_tensor:', result.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.tensor(w, dtype=torch.float64)\n",
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', v_tensor.dtype)\n",
    "print('w_tensor:', w_tensor.dtype)\n",
    "result = v_tensor @ w_tensor\n",
    "print('v_tensor @ x_tensor:', result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dérivation automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'initialisation d'un tenseur, l'argument `requires_grad=True` indique que nous désirons calculer le gradient des variables contenues dans le tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphe de calcul est alors bâti au fur et à mesure que des opérations mathématiques sont appliquées aux tenseurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `F.backward()` parcours le graphe de calcul en sens inverse et calcule le gradient de la fonction $F$ selon chacune des variables du graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir exécuté la fonction `backward()`, l'attribut `grad` des tenseurs impliqués dans le calcul contient la valeur du gradient calculé au point courant. Ici, on aura la valeur :\n",
    "\n",
    "$$\\left[\\frac{\\partial F(x)}{\\partial x}\\right]_{x=3} = \\big[\\,2\\,x\\,\\big]_{x=3} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrons le fonctionnement de la dérivation automatique par quelques autres exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 11, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produit_scalaire = x @ x\n",
    "produit_scalaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produit_scalaire.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = a*b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = 2*a + b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=False)\n",
    "m = a ** b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = torch.tensor(4., requires_grad=True)\n",
    "m1 = (a + b)\n",
    "m2 = m1 * c\n",
    "m2.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)\n",
    "print('c.grad =', c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteur_a = torch.tensor([-1., 2, 3], requires_grad=True)\n",
    "vecteur_b = torch.ones(3, requires_grad=True)\n",
    "produit = vecteur_a @ vecteur_b\n",
    "produit.backward()\n",
    "print('vecteur_a =', vecteur_a, '; vecteur_a.grad =', vecteur_a.grad)\n",
    "print('vecteur_b =', vecteur_b, '; vecteur_b.grad =', vecteur_b.grad)\n",
    "print('produit =', produit.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteur_a = torch.tensor([1., 4, 9], requires_grad=True)\n",
    "result = torch.sum(torch.sqrt(vecteur_a))\n",
    "result.backward()\n",
    "print('vecteur_a =', vecteur_a, '; vecteur_a.grad =', vecteur_a.grad)\n",
    "print('result =', result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condidération technique\n",
    "Pour convertir un *tenseur pytorch* initialisé avec `requiere_gradient=True` en *array numpy*, on doit d'abord «détacher» sa valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2,2), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import aidecours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencons par un exemple en une dimension.\n",
    "\n",
    "$$f(x) = x^2 - x + 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_maison(x):\n",
    "    return x**2 - x + 2\n",
    "\n",
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, fonction_maison(x) )\n",
    "plt.scatter(.5, fonction_maison(.5), s=150, marker='*', c='r')\n",
    "plt.ylim(0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant éffectue une descente de gradient de la «fonction maison» $f(x)$ pour $20$ itérations. Nous introduisons du même coup deux fonctionnalités de *pytorch*:\n",
    "* Le *contexte* `torch.no_grad()`, qui spécifie à pytorch qu'il n'est pas nécessaire de calculer les gradients associés aux opérations dans le bloc de code correspondant.\n",
    "* La méthode `zero_()` qui réinitialise la valeur des gradients pour d'une variable de type `Tensor`. Il est nécessaire d'y faire appel avant chacune des nouvelles itérations de la descente de gradient, sans quoi les gradienys seront additionnés les uns avec les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = .4 # Pas de gradient\n",
    "T = 20   # Nombre d'itérations\n",
    "\n",
    "# Initialisation aléatoire \n",
    "x = torch.randn(1, requires_grad=True)\n",
    "\n",
    "for t in range(T):\n",
    " \n",
    "    # Calcul de la fonction objectif\n",
    "    val = fonction_maison(x)\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    val.backward()\n",
    "    \n",
    "    print(f\"Iteration {t+1:02}:\",\n",
    "          f\" x ={x.item(): .5f}\",\n",
    "          f\" F(x) ={val.item(): .5f}\",\n",
    "          f\" F\\'(x) ={x.grad.item(): .5f}\")\n",
    "    \n",
    "    # Mise à jour de la variable x\n",
    "    with torch.no_grad():\n",
    "        x -= eta * x.grad\n",
    "    \n",
    "    # Remise à zéro du gradient\n",
    "    x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons l'exemple des moindres carrés présenté dans les transparents du cours.\n",
    "\n",
    "$$\\min_\\wbf \\left[\\frac1n \\sum_{i=1}^n (\\wbf\\cdot\\xbf_i- y_i)^2\\right].$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moindres_carres_objectif(x, y, w): \n",
    "    return np.mean((x @ w - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(1,1), (0,-1), (2,.5)])\n",
    "y = np.array([-1, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonction_objectif = lambda w: moindres_carres_objectif(x, y, w)\n",
    "aidecours.show_2d_function(fonction_objectif, -5, 5, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "\n",
    "aidecours.show_2d_function(fonction_objectif, -5, 5, .5, optimal=w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons une classe `moindres_carres` qui résout le problème des moindres carrés par descente de gradient, en utilisant les fonctionnalités de *pyTorch*.\n",
    "\n",
    "* Lors de l'initialisation de la classe (méthode `__init__`), l'utilisateur spécifie les paramètres de la descente en gradient.\n",
    "* La méthode `apprentissage` exécute la descente de gradient, qui minimise la perte quadratique sur l'ensemble d'apprentissage défini par `x` et `y`.\n",
    "* La méthode `prediction` calcule la valeur du régresseur «appris» sur l'ensemble de données `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moindres_carres:\n",
    "    def __init__(self, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation des listes enregistrant la trace de l'algorithme\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "    \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            loss = torch.mean((x @ self.w - y) ** 2)           \n",
    "            self._trace(self.w, loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    self.w -= self.eta * self.w.grad\n",
    "                \n",
    "                self.w.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w\n",
    "            \n",
    "        return pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécution de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = moindres_carres(eta, nb_iter)\n",
    "algo.apprentissage(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14.5, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, fonction_objectif, w_opt=w_opt, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1], obj_opt=fonction_objectif(w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Dans cet exercice, nous vous demandons de vous inspirer de la classe `moindre_carrees` ci-haut et de l'adapter au problème de la régression logistique présenté dans les transparents du cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "xx, yy = make_blobs(n_samples=100, centers=2, n_features=2, cluster_std=1, random_state=0)\n",
    "\n",
    "aidecours.show_2d_dataset(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrons la fonction à optimiser (avec $\\lambda=0.01$):\n",
    "    \n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i \\wbf\\cdot\\xbf_i + \\log(1+e^{\\wbf\\cdot\\xbf_i})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def calc_perte_logistique(w, x, y, rho):\n",
    "    pred = sigmoid(x @ w)\n",
    "    pred[y==0] = 1-pred[y==0]\n",
    "    return np.mean(-np.log(pred)) + rho*w @ w/2\n",
    "\n",
    "fct_objectif = lambda w: calc_perte_logistique(w, xx, yy, 0.01)\n",
    "aidecours.show_2d_function(fct_objectif, -4, 4, .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compléter le code de la classe suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.rho = rho         # Paramètre de regularisation\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation  trace de la descente de gradient\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "    \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # à compléter\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w\n",
    "                        \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter le code suivant pour vérifier le bon fonctionnement de votre algorithme. Essayer ensuite de varier les paramètres `rho`, `eta` et `nb_iter` afin d'évaluer leur impact sur le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rho = 0.01    # paramètre de régularisation\n",
    "eta = 0.5     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = regression_logistique(rho, eta, nb_iter, seed=2)\n",
    "algo.apprentissage(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, fct_objectif, (-2,-3), (3,2), .05, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo._obj_list, ax=axes[1])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[2])\n",
    "plt.scatter(0,0, marker='+', c='k', s=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons l'exercice précédent en ajoutant l'apprentissange d'un *biais* à la régression logistique:\n",
    "\n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i (\\wbf\\cdot\\xbf_i+b) + \\log(1+e^{\\wbf\\cdot\\xbf_i+b})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compléter le code de la classe suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique_avec_biais:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.rho = rho         # Paramètre de regularisation\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation  trace de la descente de gradient\n",
    "        self.w_list = list()   \n",
    "        self.b_list = list()\n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, b, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.b_list.append(b.item())\n",
    "        self.obj_list.append(obj.item()) \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "           \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # à compléter\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w + self.b\n",
    "            \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter le code suivant pour vérifier le bon fonctionnement de votre algorithme. Essayer ensuite de varier les paramètres `rho`, `eta` et `nb_iter` afin d'évaluer leur impact sur le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.01    # paramètre de régularisation\n",
    "eta = 0.4     # taille du pas\n",
    "nb_iter = 20  # nombre d'itérations\n",
    "\n",
    "algo = regression_logistique_avec_biais(rho, eta, nb_iter)\n",
    "algo.apprentissage(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[0])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[1])\n",
    "plt.scatter(0,0, marker='+', c='k', s=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

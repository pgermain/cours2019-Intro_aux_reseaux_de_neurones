{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la librairie PyTorch -- Solutions\n",
    "Matériel de cours rédigé par Pascal Germain, 2019\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        self.rho = rho\n",
    "        self.eta = eta\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        \n",
    "        self.w_list = list() # Servira à garder une trace de la descente de gradient\n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            xw = x @ self.w\n",
    "            w2 = self.w @ self.w\n",
    "            loss = torch.mean(- y * xw + torch.log(1+torch.exp(xw))) + self.rho * w2 / 2\n",
    "            \n",
    "            self.w_list.append(np.array(self.w.detach()))\n",
    "            self.obj_list.append(loss.item()) \n",
    "            if t == self.nb_iter: break \n",
    "        \n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.w -= self.eta * self.w.grad\n",
    "                \n",
    "            self.w.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = x @ self.w.detach()\n",
    "        \n",
    "        return np.array(pred.numpy() > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique_avec_biais:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        self.rho = rho\n",
    "        self.eta = eta\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        \n",
    "        self.w_list = list() # Servira à garder une trace de la descente de gradient\n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            xw = x @ self.w + self.b\n",
    "            w2 = self.w @ self.w\n",
    "            loss = torch.mean(- y * xw + torch.log(1+torch.exp(xw))) + self.rho * w2 / 2\n",
    "            \n",
    "            self.w_list.append(np.array(self.w.detach()))\n",
    "            self.obj_list.append(loss.item()) \n",
    "            if t == self.nb_iter: break \n",
    "        \n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.w -= self.eta * self.w.grad\n",
    "                self.b -= self.eta * self.b.grad\n",
    "                \n",
    "            self.w.grad.zero_()\n",
    "            self.b.grad.zero_()            \n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = x @ self.w.detach() + self.b.item()\n",
    "            \n",
    "        return np.array(pred.numpy() > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reseau_classification:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        self.nb_neurones = nb_neurones\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.nb_iter = nb_iter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.nb_neurones, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        perte_logistique = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "        \n",
    "        self.obj_list = list()\n",
    "    \n",
    "        for t in range(self.nb_iter+1):\n",
    "            y_pred = self.model(x)\n",
    "            \n",
    "            loss = perte_logistique(y_pred, y)\n",
    "                  \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            self.obj_list.append(loss.item())\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        return np.array(pred.detach() > .5, dtype=np.int)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

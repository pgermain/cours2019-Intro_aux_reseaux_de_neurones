{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la librairie PyTorch -- Solutions\n",
    "Matériel de cours rédigé par Pascal Germain, 2019\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.rho = rho         # Paramètre de regularisation\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation  trace de la descente de gradient\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "    \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            xw = x @ self.w\n",
    "            w2 = self.w @ self.w\n",
    "            loss = torch.mean(- y * xw + torch.log(1 + torch.exp(xw))) + self.rho * w2 / 2          \n",
    "            self._trace(self.w, loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    self.w -= self.eta * self.w.grad\n",
    "                \n",
    "                self.w.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w\n",
    "                        \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_logistique_avec_biais:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.rho = rho         # Paramètre de regularisation\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation  trace de la descente de gradient\n",
    "        self.w_list = list()   \n",
    "        self.b_list = list()\n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, b, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.b_list.append(b.item())\n",
    "        self.obj_list.append(obj.item()) \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "           \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            xw = x @ self.w + self.b\n",
    "            w2 = self.w @ self.w \n",
    "            loss = torch.mean(- y * xw + torch.log(1 + torch.exp(xw))) + self.rho * w2 / 2          \n",
    "            self._trace(self.w, self.b, loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    self.w -= self.eta * self.w.grad\n",
    "                    self.b -= self.eta * self.b.grad\n",
    "                    \n",
    "                self.w.grad.zero_()\n",
    "                self.b.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w + self.b\n",
    "            \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reseau_classification:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Architecture du réseau\n",
    "        self.nb_neurones = nb_neurones # Nombre de neurones sur la couche cachée\n",
    "        \n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation des listes enregistrant la trace de l'algorithme\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, obj):\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.nb_neurones, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        perte_logistique = nn.BCELoss()\n",
    "        optimiseur = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            y_pred = self.model(x)\n",
    "            perte = perte_logistique(y_pred, y)         \n",
    "            self._trace(perte)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                perte.backward()\n",
    "                optimiseur.step()\n",
    "                optimiseur.zero_grad()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        pred = pred.squeeze()\n",
    "        return np.array(pred > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

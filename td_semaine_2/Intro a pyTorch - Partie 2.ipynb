{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction à la librairie PyTorch -- Partie 2\n",
    "Matériel de cours rédigé par Pascal Germain, 2019\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import aidecours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le module `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module `nn` de la librairie `torch` contient plusieurs outils pour construire l'architecture d'un réseau de neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons l'exemple des moindres carrés de la partie précédente, afin de montrer comment exprimer le problème sous la forme d'une réseau de neurones avec les outils qu'offrent *pyTorch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation des données\n",
    "Préparons les données d'apprentissage sous la forme de *tenseurs pyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(1,1),(0,-1),(2,.5)])\n",
    "y = np.array([-1., 3, 2])\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = y_tensor.unsqueeze(1) # Les méthodes du module torch.nn sont conçues pour manipuler des matrices\n",
    "y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Couche linéaire\n",
    "\n",
    "La classe `Linear` correspond à une *couche* linéaire. La méthode des moindres carrés nécessite seulement un neurone de sortie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone = nn.Linear(2, 1, bias=False)\n",
    "neurone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurone(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MSELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perte_quadratique = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perte_quadratique(neurone(x_tensor), y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module d'optimisation `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4\n",
    "alpha = 0.1\n",
    "\n",
    "neurone = nn.Linear(2, 1, bias=False)\n",
    "optimiseur = torch.optim.SGD(neurone.parameters(), lr=eta, momentum=alpha)\n",
    "\n",
    "for t in range(20):\n",
    "\n",
    "    y_pred = neurone(x_tensor)                   # Calcul de la sortie de la neurone\n",
    "    loss = perte_quadratique(y_pred, y_tensor)   # Calcul de la fonction de perte\n",
    "    loss.backward()                              # Calcul des gradients\n",
    "    optimiseur.step()                            # Effectue un pas de la descente de gradient\n",
    "    optimiseur.zero_grad()                       # Remet à zero les variables du gradient\n",
    "    \n",
    "    print(t, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joignons tout cela ensemble afin de réécrire le module `moindres_carres` avec les outils de *pyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moindres_carres:\n",
    "    def __init__(self, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation des listes enregistrant la trace de l'algorithme\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.squeeze().detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.neurone = nn.Linear(d, 1, bias=False)\n",
    "        perte_quadratique = nn.MSELoss()\n",
    "        optimiseur = torch.optim.SGD(self.neurone.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            y_pred = self.neurone(x)\n",
    "            perte = perte_quadratique(y_pred, y)         \n",
    "            self._trace(self.neurone.weight, perte)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                perte.backward()\n",
    "                optimiseur.step()\n",
    "                optimiseur.zero_grad()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.neurone(x)\n",
    "            \n",
    "        return pred.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4      # taille du pas\n",
    "alpha = 0.0    # momentum\n",
    "nb_iter = 20   # nombre d'itérations\n",
    "\n",
    "algo = moindres_carres(eta, alpha, nb_iter, seed=None)\n",
    "algo.apprentissage(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.prediction(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14.5, 4))\n",
    "fonction_objectif = lambda w: np.mean((x @ w - y) ** 2)\n",
    "aidecours.show_2d_trajectory(algo.w_list, fonction_objectif, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1], obj_opt=fonction_objectif(w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout d'une couche cachée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couche_cachee = nn.Linear(2, 4)\n",
    "couche_cachee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couche_cachee.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couche_cachee.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variables in couche_cachee.parameters():\n",
    "    print(variables)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couche_cachee(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions d'activations\n",
    "Fonction d'activation *ReLU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(-2, 2, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu(couche_cachee(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'activation *tanh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Tanh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh = nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh(couche_cachee(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'activation *sigmoïdale*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sigmoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoide = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoide(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoide(couche_cachee(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Succession de couches et de fonctions d'activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    torch.nn.Linear(2, 4),\n",
    "    torch.nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    torch.nn.Linear(2, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variables in model.parameters():\n",
    "    print(variables)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau de neurones à une couche cachée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reseau_regression:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Architecture du réseau\n",
    "        self.nb_neurones = nb_neurones # Nombre de neurones sur la couche cachée\n",
    "        \n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation des listes enregistrant la trace de l'algorithme\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, obj):\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.nb_neurones, 1)\n",
    "        )\n",
    "        \n",
    "        perte_quadratique = nn.MSELoss()\n",
    "        optimiseur = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            y_pred = self.model(x)\n",
    "            perte = perte_quadratique(y_pred, y)         \n",
    "            self._trace(perte)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                perte.backward()\n",
    "                optimiseur.step()\n",
    "                optimiseur.zero_grad()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        return pred.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurones = 4\n",
    "eta = 0.1      # taille du pas\n",
    "alpha = 0.1    # momentum\n",
    "nb_iter = 50   # nombre d'itérations\n",
    "\n",
    "x = np.array([(1,1),(0,-1),(2,.5)])\n",
    "y = np.array([-1., 3, 2])\n",
    "\n",
    "algo = reseau_regression(nb_neurones, eta, alpha, nb_iter, seed=None)\n",
    "algo.apprentissage(x, y)\n",
    "\n",
    "aidecours.show_learning_curve(algo.obj_list)\n",
    "predictions = algo.prediction(x)\n",
    "print('y    =', y)\n",
    "print('R(x) =', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cet exercice est d'adapter la classe `reseau_regression` présentée plus haut pour résoudre le problème de *classification* suivant.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "xx, yy = make_circles(n_samples=100, noise=.1, factor=0.2, random_state=10)\n",
    "aidecours.show_2d_dataset(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous demandons de compléter la fonction `fit` de la classe `reseau_classification` ci-bas. Nous vous conseillons de vous inspirer de la régression logistique en utilisant une fonction d'activation *sigmoïdale* en sortie, ainsi que la perte du **négatif log vraisemblance**. Il n'est pas nécessaire d'ajouter un terme de régularisation au réseau.\n",
    "\n",
    "**Notez bien**: La fonction de perte du **négatif log vraisemblance** vue en classe correspond à la classe `nn.BCELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reseau_classification:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Architecture du réseau\n",
    "        self.nb_neurones = nb_neurones # Nombre de neurones sur la couche cachée\n",
    "        \n",
    "        # Initialisation des paramètres de la descente en gradient\n",
    "        self.eta = eta         # Pas de gradient\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Nombre d'itérations\n",
    "        self.seed = seed       # Germe du générateur de nombres aléatoires\n",
    "        \n",
    "        # Initialisation des listes enregistrant la trace de l'algorithme\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, obj):\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def apprentissage(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            # Compléter l'architecture ici\n",
    "        )\n",
    "        \n",
    "        perte_logistique = nn.BCELoss()\n",
    "        optimiseur = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # Compléter l'apprentissage ici\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        pred = pred.squeeze()\n",
    "        return np.array(pred > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter le code suivant pour tester votre réseau. Varier les paramètres pour mesurer leur influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurones = 10\n",
    "eta = 0.6     # taille du pas\n",
    "alpha = 0.4   # momentum\n",
    "nb_iter = 50  # nombre d'itérations\n",
    "\n",
    "algo = reseau_classification(nb_neurones, eta, alpha, nb_iter)\n",
    "algo.apprentissage(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[0])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nous vous suggérons d'explorer le comportement du réseau en:\n",
    "1. Modifiant la fonction d'activation *ReLU* pour une fonction d'activation *tanh*\n",
    "2. Ajoutant une ou plusieurs autres couches cachées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction to PyTorch -- Part 2\n",
    "Pascal Germain, 2019\n",
    "Vera Shalaeva (translated to English), 2020\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import aidecours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le module `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__ # This notebook works with the pytorch version'1.5.1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `nn` of the library `torch` content many tools to create neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the example of least squares method from the Part1 and represent it in the form of a neural network learning problem by using *pyTorch* library tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "Let's prepare the learning data in form of *pyTorch tensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(1,1),(0,-1),(2,.5)])\n",
    "y = np.array([-1., 3, 2])\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = y_tensor.unsqueeze(1) # Methods of torch.nn module are designed to manipulate matrices.\n",
    "y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layer\n",
    "\n",
    "The class `Linear` corresponds to a *hidden* layer. The least squares method requires only one output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = nn.Linear(2, 1, bias=False)\n",
    "neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MSELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_loss(neuron(x_tensor), y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization module `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4\n",
    "alpha = 0.1\n",
    "\n",
    "neuron = nn.Linear(2, 1, bias=False)\n",
    "optimizer = torch.optim.SGD(neuron.parameters(), lr=eta, momentum=alpha)\n",
    "\n",
    "for t in range(20):\n",
    "\n",
    "    y_pred = neuron(x_tensor)                   # Computes output of the neuron\n",
    "    loss = quadratic_loss(y_pred, y_tensor)     # Computes the loss value\n",
    "    loss.backward()                             # Computes gradients\n",
    "    optimizer.step()                           # Does one step of the gradient descent\n",
    "    optimizer.zero_grad()                      # Reinitialize gradient values to zero \n",
    "    \n",
    "    print(t, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the class `least_squares` by using *pyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class least_squares:\n",
    "    def __init__(self, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Initialization of parameters of gradient descent\n",
    "        self.eta = eta         # Gradient step value\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of random number generator\n",
    "        \n",
    "        # Initialization of the path of the gradien descent\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.squeeze().detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def training(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.neuron = nn.Linear(d, 1, bias=False)\n",
    "        quadratic_loss = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.neuron.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            y_pred = self.neuron(x)\n",
    "            loss = quadratic_loss(y_pred, y)         \n",
    "            self._trace(self.neuron.weight, loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.neuron(x)\n",
    "            \n",
    "        return pred.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4      # gradient step\n",
    "alpha = 0.0    # momentum\n",
    "nb_iter = 20   # number of iterations\n",
    "\n",
    "algo = least_squares(eta, alpha, nb_iter, seed=None)\n",
    "algo.training(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.prediction(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14.5, 4))\n",
    "objective_function = lambda w: np.mean((x @ w - y) ** 2)\n",
    "aidecours.show_2d_trajectory(algo.w_list, objective_function, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1], obj_opt=objective_function(w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = nn.Linear(2, 4)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variables in hidden_layer.parameters():\n",
    "    print(variables)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function\n",
    "Activation function *ReLU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(-2, 2, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu(hidden_layer(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function *tanh*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Tanh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh = nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_tanh(hidden_layer(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation fucntion *sigmoid*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sigmoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sigmoid(hidden_layer(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence of layers and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    torch.nn.Linear(2, 4),\n",
    "    torch.nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    torch.nn.Linear(2, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variables in model.parameters():\n",
    "    print(variables)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_network:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Network architecture\n",
    "        self.nb_neurones = nb_neurones # Number of neurons of the hidden layer\n",
    "        \n",
    "        # Initialization of parameters of the gradient descent\n",
    "        self.eta = eta         # Gradient step\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of the random number generator\n",
    "        \n",
    "        # Initialization of the path of the gradient descent\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, obj):\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def training(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.nb_neurones, 1)\n",
    "        )\n",
    "        \n",
    "        quadratic_loss = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            y_pred = self.model(x)\n",
    "            loss = quadratic_loss(y_pred, y)         \n",
    "            self._trace(loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        return pred.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurones = 4\n",
    "eta = 0.1      # gradient step\n",
    "alpha = 0.1    # momentum\n",
    "nb_iter = 50   # number of iterations\n",
    "\n",
    "x = np.array([(1,1),(0,-1),(2,.5)])\n",
    "y = np.array([-1., 3, 2])\n",
    "\n",
    "algo = regression_network(nb_neurones, eta, alpha, nb_iter, seed=None)\n",
    "algo.training(x, y)\n",
    "\n",
    "aidecours.show_learning_curve(algo.obj_list)\n",
    "predictions = algo.prediction(x)\n",
    "print('y    =', y)\n",
    "print('R(x) =', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to modify the class `reseau_regression` above that it will be able to solve the classification problem below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "xx, yy = make_circles(n_samples=100, noise=.1, factor=0.2, random_state=10)\n",
    "aidecours.show_2d_dataset(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `fit` of the class `classification network` below. We recommend that you use the *sigmoid* activation function, as well as **negative log likelihood** loss function. It is not necessary to add the regularization term to the network.\n",
    "\n",
    "**Note**: The **negative log likelihood** loss function correspondsto the class `nn.BCELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_net:\n",
    "    def __init__(self, nb_neurones=4, eta=0.4, alpha=0.1, nb_iter=50, seed=None):\n",
    "        # Network architecture\n",
    "        self.nb_neurones = nb_neurones # Number of neuroned in a hidden layer\n",
    "        \n",
    "        # Initialization of the parameters of the gradient descent\n",
    "        self.eta = eta         # Gradient step\n",
    "        self.alpha = alpha     # Momentum\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of random number generator\n",
    "        \n",
    "        # Initialization of the path of the gradient descent\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, obj):\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        n, d = x.shape\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(d, self.nb_neurones),\n",
    "            # Complete the architecture\n",
    "        )\n",
    "        \n",
    "        logistic_loss = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.eta, momentum=self.alpha)\n",
    "                   \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # Complete \n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            \n",
    "        pred = pred.squeeze()\n",
    "        return np.array(pred > .5, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to test your network. Try to change parameters to get insights of their influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurones = 10\n",
    "eta = 0.6     # gradient step\n",
    "alpha = 0.4   # momentum\n",
    "nb_iter = 50  # number of iterations\n",
    "\n",
    "algo = classification_net(nb_neurones, eta, alpha, nb_iter)\n",
    "algo.fit(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[0])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we suggest that you explore the network by: \n",
    "1. Changing the activation function *ReLU* by the activation function *tanh*\n",
    "2. Adding one or many hidden layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

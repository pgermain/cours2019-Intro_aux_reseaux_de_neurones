{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction to PyTorch -- Part1\n",
    "Pascal Germain, 2019\n",
    "Vera Shalaeva(translation to English), 2020\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__ # This notebook works with pytorch version 1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar\n",
    "A tensor can contain a scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors\n",
    "Tensors that comprise of vectors or matrices can be manipulated in the same way as *array numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1,2,3])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([1.,2.,3.])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([[1.,2.,3.], [4, 5, 6]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * M + 1 # Ariphmetic operations on a matrix elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M @ u # Produit matriciel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(5) # Random value drawn from uniform distribution on the interval [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(8) # Random value drawn from Gaussian distribution N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((3, 4)) # Random vector drawn from uniform distribution on the interval [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((3, 4)) # Random vector drawn from Gaussian distribution N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Initialization of random number generator\n",
    "torch.randn((3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion between numpy and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to do ariphmetic operations directly between an *array numpy* and a *tensor pytorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-1, 3, 8])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u = torch.tensor([1,2,3])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w + u # Implies an error with pytorch 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u + w # Implies an error with pytorch 1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longrightarrow$ pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.from_numpy(w)\n",
    "w_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longleftarrow$ pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_numpy = u.numpy()  # Conversion d'un «tensor» pytorch en un «array» numpy. \n",
    "u_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical point #1\n",
    "In two previous exemples, the data structures share the same memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w += 1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u *= 2\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longleftarrow$ pytorch ***with a copy in memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_tensor = torch.Tensor(w)\n",
    "ww_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w +=1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion numpy $\\Longrightarrow$ pytorch ***with a copy in memory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_numpy_copy = np.array(u) # Conversion of a pytorch tensor to an «array» numpy. \n",
    "u_numpy_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u *= 2\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_numpy_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical point #2\n",
    "Manipulations on variables having different data types are more tricky for pytorch tensors than for array numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([.3, .6, .9])\n",
    "v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-1, 3, 8])\n",
    "w.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tensor = torch.from_numpy(v)\n",
    "v_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.from_numpy(w)\n",
    "w_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v:', v.dtype)\n",
    "print('w:', w.dtype)\n",
    "\n",
    "result = v @ w\n",
    "print('v @ w:', result.dtype)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', w_tensor.dtype)\n",
    "print('w_tensor:', v_tensor.dtype)\n",
    "result = v_tensor @ w_tensor  # Implies an error with pytorch 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tensor = torch.tensor(w, dtype=torch.float64)\n",
    "w_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v_tensor:', v_tensor.dtype)\n",
    "print('w_tensor:', w_tensor.dtype)\n",
    "result = v_tensor @ w_tensor\n",
    "print('v_tensor @ x_tensor:', result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During a tensor initialization, an argument `requires_grad=True` indicates that we want to compute the gradient of variables the tensor contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph is built gradually as mathematical opertations are applied to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `F.backward()` walks the computational graph in backward direction and compute the gradient of the function $F$ according to each variable of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the function`backward()`, the applied method `grad` to the tensor return gradient values calculated at the current point. Here, we have the following value:\n",
    "\n",
    "$$\\left[\\frac{\\partial F(x)}{\\partial x}\\right]_{x=3} = \\big[\\,2\\,x\\,\\big]_{x=3} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the other examples of automatic derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 11, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_product = x @ x\n",
    "scalar_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_product.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = a*b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "m = 2*a + b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=False)\n",
    "m = a ** b\n",
    "m.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-3., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = torch.tensor(4., requires_grad=True)\n",
    "m1 = (a + b)\n",
    "m2 = m1 * c\n",
    "m2.backward()\n",
    "print('a.grad =', a.grad)\n",
    "print('b.grad =', b.grad)\n",
    "print('c.grad =', c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_a = torch.tensor([-1., 2, 3], requires_grad=True)\n",
    "vector_b = torch.ones(3, requires_grad=True)\n",
    "product = vector_a @ vector_b\n",
    "product.backward()\n",
    "print('vector_a =', vector_a, '; vector_a.grad =', vector_a.grad)\n",
    "print('vector_b =', vector_b, '; vector_b.grad =', vector_b.grad)\n",
    "print('product =', product.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_a = torch.tensor([1., 4, 9], requires_grad=True)\n",
    "result = torch.sum(torch.sqrt(vector_a))\n",
    "result.backward()\n",
    "print('vector_a =', vector_a, '; vector_a.grad =', vector_a.grad)\n",
    "print('result =', result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical details\n",
    "To convert a *pytorch tensor* initialized with `requiere_gradient=True` to a *array numpy*, we have to first **detach** its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2,2), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.numpy() # Implies an error in pytorch 1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradien descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import aidecours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an exemple in one dimension.\n",
    "\n",
    "$$f(x) = x^2 - x + 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_d_function(x):\n",
    "    return x**2 - x + 2\n",
    "\n",
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, one_d_function(x) )\n",
    "plt.scatter(.5, one_d_function(.5), s=150, marker='*', c='r')\n",
    "plt.ylim(0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code applies the gradient descend method to the \"one_d_function\" $f(x)$ for $20$ iterations. Consider two methods of *pytorch* we will use below:\n",
    "* The *context* `torch.no_grad()` that specifies that it is not necessary to compute gradients associated to the operations in the corresponding bloack of code.\n",
    "* The method `zero_()` that reinitialize the gradient values for variables of type `Tensor`. It is necessary to call it before each new iteration, otherwise gradient value will be accumulated over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = .4 # Gradient step\n",
    "T = 20   # Number of iterations\n",
    "\n",
    "# Random initialization \n",
    "x = torch.randn(1, requires_grad=True)\n",
    "\n",
    "for t in range(T):\n",
    " \n",
    "    # Compute the objective function\n",
    "    val = one_d_function(x)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    val.backward()\n",
    "    \n",
    "    print(f\"Iteration {t+1:02}:\",\n",
    "          f\" x ={x.item(): .5f}\",\n",
    "          f\" F(x) ={val.item(): .5f}\",\n",
    "          f\" F\\'(x) ={x.grad.item(): .5f}\")\n",
    "    \n",
    "    # Update the variable x\n",
    "    with torch.no_grad():\n",
    "        x -= eta * x.grad\n",
    "    \n",
    "    # Reinitialize gradient to zero\n",
    "    x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the example of the least squares method we considered earlier in the course.\n",
    "\n",
    "$$\\min_\\wbf \\left[\\frac1n \\sum_{i=1}^n (\\wbf\\cdot\\xbf_i- y_i)^2\\right].$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_objective(x, y, w): \n",
    "    return np.mean((x @ w - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(1,1), (0,-1), (2,.5)])\n",
    "y = np.array([-1, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = lambda w: least_squares_objective(x, y, w)\n",
    "aidecours.show_2d_function(objective_function, -5, 5, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = np.linalg.inv(x.T @ x) @ x.T @ y\n",
    "\n",
    "aidecours.show_2d_function(objective_function, -5, 5, .5, optimal=w_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class `least_squares`, which resolve the least squares problem by gradient descend by using *pytorch* library.\n",
    "\n",
    "* During initialization of the class (method `__init__`), a user specifies parameters of the gradient descend. \n",
    "* The method `training` runs the gradient descent, which minimizes the quadratic loss on the given training data `x` and `y`.\n",
    "* The method `prediction` compute the value of the predictor learnt on the training data `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class least_squares:\n",
    "    def __init__(self, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialization of parameters of the gradient descent\n",
    "        self.eta = eta         # Gradient step\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of random number generator\n",
    "        \n",
    "        # Initialization of the lists to track the algorithm's path\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def training(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "    \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            loss = torch.mean((x @ self.w - y) ** 2)           \n",
    "            self._trace(self.w, loss)\n",
    "  \n",
    "            if t < self.nb_iter:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    self.w -= self.eta * self.w.grad\n",
    "                \n",
    "                self.w.grad.zero_()\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w\n",
    "            \n",
    "        return pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.4     # Gradient step value\n",
    "nb_iter = 20  # Number of iterations\n",
    "\n",
    "algo = least_squares(eta, nb_iter)\n",
    "algo.training(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14.5, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, objective_function, w_opt=w_opt, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1], obj_opt=objective_function(w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersise\n",
    "In this exercise, you are asked to write the class adopted to the problem of logistic regression by using and modify the class `least_squares` given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "xx, yy = make_blobs(n_samples=100, centers=2, n_features=2, cluster_std=1, random_state=0)\n",
    "\n",
    "aidecours.show_2d_dataset(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the objective function ($\\lambda=0.01$):\n",
    "    \n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i \\wbf\\cdot\\xbf_i + \\log(1+e^{\\wbf\\cdot\\xbf_i})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(w, x, y, rho):\n",
    "    pred = sigmoid(x @ w)\n",
    "    pred[y==0] = 1-pred[y==0]\n",
    "    return np.mean(-np.log(pred)) + rho*w @ w/2\n",
    "\n",
    "fct_objective = lambda w: logistic_loss(w, xx, yy, 0.01)\n",
    "aidecours.show_2d_function(fct_objective, -4, 4, .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the code of the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialization of parameters of the gradient descent\n",
    "        self.rho = rho         # Regularization parameter\n",
    "        self.eta = eta         # Gradient step\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of the random number generator\n",
    "        \n",
    "        # Initialization of the gradient descent path\n",
    "        self.w_list = list()   \n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.obj_list.append(obj.item())      \n",
    "        \n",
    "    def training(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "    \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # to complete\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w\n",
    "                        \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify the correct functionning of your algorithm. Try to change parameters `rho`, `eta` and `nb_iter` in order to evaluate their impact on the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rho = 0.01    # regularization parameter\n",
    "eta = 0.5     # gradient step value\n",
    "nb_iter = 20  # number of iterations\n",
    "\n",
    "algo = logistic_regression(rho, eta, nb_iter, seed=2)\n",
    "algo.training(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "aidecours.show_2d_trajectory(algo.w_list, fct_objective, (-2,-3), (3,2), .05, ax=axes[0])\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[1])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[2])\n",
    "plt.scatter(0,0, marker='+', c='k', s=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the *bias* term to the training of the logistic regressor:\n",
    "\n",
    "$$\n",
    "\\frac1n \\sum_{i=1}^n - y_i (\\wbf\\cdot\\xbf_i+b) + \\log(1+e^{\\wbf\\cdot\\xbf_i+b})+ \\frac\\rho2\\|\\wbf\\|^2\\,.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_with_bias:\n",
    "    def __init__(self, rho=.01, eta=0.4, nb_iter=50, seed=None):\n",
    "        # Initialization of parameters of the gradient descent \n",
    "        self.rho = rho         # Regularization parameter\n",
    "        self.eta = eta         # Gradient step\n",
    "        self.nb_iter = nb_iter # Number of iterations\n",
    "        self.seed = seed       # Seed of the random number generator\n",
    "        \n",
    "        # Initialization of the gradient descent path\n",
    "        self.w_list = list()   \n",
    "        self.b_list = list()\n",
    "        self.obj_list = list()\n",
    "        \n",
    "    def _trace(self, w, b, obj):\n",
    "        self.w_list.append(np.array(w.detach()))\n",
    "        self.b_list.append(b.item())\n",
    "        self.obj_list.append(obj.item()) \n",
    "        \n",
    "    def training(self, x, y):\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32) \n",
    "\n",
    "        n, d = x.shape\n",
    "        self.w = torch.randn(d, requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "           \n",
    "        for t in range(self.nb_iter + 1):\n",
    "            pass # to complete\n",
    "                \n",
    "    def prediction(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = x @ self.w + self.b\n",
    "            \n",
    "        return np.array(pred.numpy() > 0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify the correct functionning of your algorithm. Try to change parameters  `rho`, `eta` and `nb_iter`  in order to evaluate their impact on the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.01    # regularization parameter\n",
    "eta = 0.4     # gradient step value\n",
    "nb_iter = 20  # number of iterations\n",
    "\n",
    "algo = logistic_regression_with_bias(rho, eta, nb_iter)\n",
    "algo.training(xx, yy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "aidecours.show_learning_curve(algo.obj_list, ax=axes[0])\n",
    "aidecours.show_2d_predictions(xx, yy, algo.prediction, ax=axes[1])\n",
    "plt.scatter(0,0, marker='+', c='k', s=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
